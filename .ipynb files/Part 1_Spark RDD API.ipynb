{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing PySpark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing neccessary libraries\n",
    "\n",
    "import csv\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#creating the SparkSession\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1_Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data file from the given location and make it accessible to Spark. Display first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading given groceries.csv dataset into Spark RDD\n",
    "\n",
    "load_rdd = sc.textFile(\"groceries.csv\")\n",
    "groceries_rdd = load_rdd.mapPartitions(lambda x: csv.reader(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups'],\n",
       " ['tropical fruit', 'yogurt', 'coffee'],\n",
       " ['whole milk'],\n",
       " ['pip fruit', 'yogurt', 'cream cheese ', 'meat spreads'],\n",
       " ['other vegetables',\n",
       "  'whole milk',\n",
       "  'condensed milk',\n",
       "  'long life bakery product']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying first 5 rows of a dataset\n",
    "\n",
    "groceries_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9835"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying total number of rows in a dataset\n",
    "\n",
    "groceries_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1_Task 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) Using Spark's RDD API, create a list of all (unique) products present in the transactions. Write out this list to a text file: out_1_2a.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of all unique products in the dataset\n",
    "\n",
    "uniqProd_rdd = groceries_rdd.flatMap(lambda x: x).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming: the question was asked to create list of all unique products\n",
    "\n",
    "uniqProd_list = uniqProd_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups', 'tropical fruit', 'yogurt', 'coffee', 'whole milk', 'pip fruit', 'cream cheese ', 'meat spreads', 'other vegetables', 'condensed milk', 'long life bakery product', 'butter', 'rice', 'abrasive cleaner', 'rolls/buns', 'UHT-milk', 'bottled beer', 'liquor (appetizer)', 'pot plants', 'cereals', 'white bread', 'bottled water', 'chocolate', 'curd', 'flour', 'dishes', 'beef', 'frankfurter', 'soda', 'chicken', 'sugar', 'fruit/vegetable juice', 'newspapers', 'packaged fruit/vegetables', 'specialty bar', 'butter milk', 'pastry', 'processed cheese', 'detergent', 'root vegetables', 'frozen dessert', 'sweet spreads', 'salty snack', 'waffles', 'candy', 'bathroom cleaner', 'canned beer', 'sausage', 'brown bread', 'shopping bags', 'beverages', 'hamburger meat', 'spices', 'hygiene articles', 'napkins', 'pork', 'berries', 'whipped/sour cream', 'artif. sweetener', 'grapes', 'dessert', 'zwieback', 'domestic eggs', 'spread cheese', 'misc. beverages', 'hard cheese', 'cat food', 'ham', 'turkey', 'baking powder', 'pickled vegetables', 'oil', 'chewing gum', 'chocolate marshmallow', 'ice cream', 'frozen vegetables', 'canned fish', 'seasonal products', 'curd cheese', 'red/blush wine', 'frozen potato products', 'specialty fat', 'specialty chocolate', 'candles', 'flower (seeds)', 'sparkling wine', 'salt', 'frozen meals', 'canned vegetables', 'onions', 'herbs', 'white wine', 'brandy', 'photo/film', 'sliced cheese', 'pasta', 'softener', 'cling film/bags', 'fish', 'male cosmetics', 'canned fruit', 'Instant food products', 'soft cheese', 'honey', 'dental care', 'popcorn', 'cake bar', 'snack products', 'flower soil/fertilizer', 'specialty cheese', 'finished products', 'cocoa drinks', 'dog food', 'prosecco', 'frozen fish', 'make up remover', 'cleaner', 'female sanitary products', 'dish cleaner', 'cookware', 'meat', 'tea', 'mustard', 'house keeping products', 'skin care', 'potato products', 'liquor', 'pet care', 'soups', 'rum', 'salad dressing', 'sauces', 'vinegar', 'soap', 'hair spray', 'instant coffee', 'roll products ', 'mayonnaise', 'rubbing alcohol', 'syrup', 'liver loaf', 'baby cosmetics', 'organic products', 'nut snack', 'kitchen towels', 'frozen chicken', 'light bulbs', 'ketchup', 'jam', 'decalcifier', 'nuts/prunes', 'liqueur', 'organic sausage', 'cream', 'toilet cleaner', 'specialty vegetables', 'baby food', 'pudding powder', 'tidbits', 'whisky', 'frozen fruits', 'bags', 'cooking chocolate', 'sound storage medium', 'kitchen utensil', 'preservation products']\n"
     ]
    }
   ],
   "source": [
    "#displaying a list of all unique products from the list created\n",
    "\n",
    "print(uniqProd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Python, writing the above list to a text file: out_1_2a.txt.\n",
    "\n",
    "with open(\"out_1_2a.txt\", 'w') as output:\n",
    "    for row in uniqProd_list:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1_Task 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Again, using Spark only, write out the total count of products to a text file: out_1_2b.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding total count of unique products\n",
    "\n",
    "uniqProd_count = uniqProd_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "#displaying total count of unique products\n",
    "\n",
    "print(uniqProd_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing total count of unique products to a text file: out_1_2a.txt\n",
    "\n",
    "spark.sparkContext.parallelize([f\"Count:\\n\" + str(uniqProd_count)]).repartition(1).saveAsTextFile(\"out_1_2b.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1_Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD and using Spark APIs, determine the top 5 purchased products along with how often they were purchased (frequency count). Write the results out in descending order of frequency into a file: out_1_3.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatMap() transformation flattens the RDD/DataFrame after applying the function on every element.\n",
    "\n",
    "flatten_rdd = groceries_rdd.flatMap(lambda x: x)\n",
    "\n",
    "#adding a new element with value 1 for each element using map() and merging the values of each element using reduceByKey()\n",
    "\n",
    "wordsMap_rdd = flatten_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the counts by higher order to lower using sortBy()\n",
    "\n",
    "sortedCounts_rdd = wordsMap_rdd.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting top 5 most purchased products along with purchased count\n",
    "\n",
    "top5purchased = sortedCounts_rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the top 5 results out in descending order of frequency into a file out_1_3.txt\n",
    "\n",
    "with open(\"out_1_3.txt\", 'w') as output:\n",
    "    for row in top5purchased:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                  End of Part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
